{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "data_dir = '../data/'\n",
    "model_dir = '../checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class FeatureSpec:\n",
    "    channel: int\n",
    "    shapelet_idx: int\n",
    "    shapelet_len: int\n",
    "\n",
    "\n",
    "def _zscore(x: np.ndarray, axis=None, eps: float = 1e-8) -> np.ndarray:\n",
    "    m = x.mean(axis=axis, keepdims=True)\n",
    "    s = x.std(axis=axis, keepdims=True)\n",
    "    return (x - m) / (s + eps)\n",
    "\n",
    "\n",
    "def min_sliding_distance(\n",
    "    x: np.ndarray,\n",
    "    s: np.ndarray,\n",
    "    per_window_z: bool = False,\n",
    "    eps: float = 1e-8,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Minimal Euclidean distance between a 1D time series x (len T)\n",
    "    and a shapelet s (len L), computed over all T-L+1 windows.\n",
    "\n",
    "    If per_window_z=True, each window and the shapelet are z-normalized\n",
    "    before computing distances (useful for amplitude/offset invariance).\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    L = s.shape[0]\n",
    "    if L > T:\n",
    "        return np.inf  # cannot slide shapelet longer than the series\n",
    "\n",
    "    # rolling windows view: shape (T-L+1, L)\n",
    "    W = sliding_window_view(x, L)\n",
    "\n",
    "    if per_window_z:\n",
    "        # z-normalize each window (row-wise) and the shapelet once\n",
    "        Wn = _zscore(W, axis=1, eps=eps)\n",
    "        sn = _zscore(s, axis=0, eps=eps)\n",
    "        # broadcast subtract → (T-L+1, L)\n",
    "        d2 = np.square(Wn - sn).sum(axis=1)\n",
    "    else:\n",
    "        d2 = np.square(W - s).sum(axis=1)\n",
    "\n",
    "    return float(np.sqrt(d2.min()))\n",
    "\n",
    "\n",
    "def build_shapelet_features(\n",
    "    X: np.ndarray,                        # (n_samples, n_channels, n_timepoints)\n",
    "    shapelets: List[List[np.ndarray]],    # list over channels -> list of 1D arrays\n",
    "    *,\n",
    "    per_window_z: bool = True,\n",
    "    global_channel_z: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[np.ndarray, List[FeatureSpec]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      F: (n_samples, n_features) shapelet-distance features\n",
    "      specs: list mapping feature index -> (channel, shapelet_idx, shapelet_len)\n",
    "    \"\"\"\n",
    "    n_samples, n_channels, n_time = X.shape\n",
    "    assert len(shapelets) == n_channels, \"Provide shapelets per channel.\"\n",
    "\n",
    "    # Optional: z-normalize each channel globally per sample (not window-wise).\n",
    "    # This de-biases scale differences before the per-window computation.\n",
    "    Xn = X.copy()\n",
    "    if global_channel_z:\n",
    "        for i in range(n_samples):\n",
    "            for c in range(n_channels):\n",
    "                Xn[i, c] = _zscore(Xn[i, c])\n",
    "\n",
    "    # Precompute total features and a spec map\n",
    "    specs: List[FeatureSpec] = []\n",
    "    for c in range(n_channels):\n",
    "        for j, shp in enumerate(shapelets[c]):\n",
    "            specs.append(FeatureSpec(channel=c, shapelet_idx=j, shapelet_len=len(shp)))\n",
    "    n_features = len(specs)\n",
    "\n",
    "    if verbose:\n",
    "        total_shapelets = sum(len(lst) for lst in shapelets)\n",
    "        print(f\"[build] samples={n_samples}, channels={n_channels}, time={n_time}\")\n",
    "        print(f\"[build] total shapelets={total_shapelets}, features={n_features}\")\n",
    "        if per_window_z:\n",
    "            print(\"[build] distance = min Euclidean on z-scored windows\")\n",
    "        elif global_channel_z:\n",
    "            print(\"[build] distance = min Euclidean (channels globally z-scored)\")\n",
    "        else:\n",
    "            print(\"[build] distance = min Euclidean (raw)\")\n",
    "\n",
    "    F = np.empty((n_samples, n_features), dtype=np.float32)\n",
    "\n",
    "    # Compute features\n",
    "    # Feature index traverses channel-major then shapelet index\n",
    "    f_idx = 0\n",
    "    for c in range(n_channels):\n",
    "        S_c = shapelets[c]\n",
    "        if verbose:\n",
    "            print(f\"[build] channel {c}: {len(S_c)} shapelets\")\n",
    "        for j, shp in enumerate(S_c):\n",
    "            # ensure 1D float array\n",
    "            s = np.asarray(shp, dtype=float).ravel()\n",
    "            for i in range(n_samples):\n",
    "                x = np.asarray(Xn[i, c], dtype=float).ravel()\n",
    "                F[i, f_idx] = min_sliding_distance(x, s, per_window_z=per_window_z)\n",
    "            f_idx += 1\n",
    "\n",
    "    return F, specs\n",
    "\n",
    "\n",
    "def summarize_top_features(\n",
    "    importances: np.ndarray,\n",
    "    specs: List[FeatureSpec],\n",
    "    k: int = 20,\n",
    "    title: str = \"Top features\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    idx = np.argsort(importances)[::-1]  # descending\n",
    "    out = []\n",
    "    for r in idx[:k]:\n",
    "        spec = specs[r]\n",
    "        out.append({\n",
    "            \"rank\": len(out) + 1,\n",
    "            \"feature_index\": int(r),\n",
    "            \"channel\": spec.channel,\n",
    "            \"shapelet_idx\": spec.shapelet_idx,\n",
    "            \"shapelet_len\": spec.shapelet_len,\n",
    "            \"importance\": float(importances[r]),\n",
    "        })\n",
    "    print(f\"\\n{title} (top {min(k, len(importances))}):\")\n",
    "    for row in out:\n",
    "        print(\n",
    "            f\"#{row['rank']:>2}  f={row['feature_index']:>4}  \"\n",
    "            f\"[ch={row['channel']}, shp={row['shapelet_idx']}, L={row['shapelet_len']}]  \"\n",
    "            f\"imp={row['importance']:.6f}\"\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def holdout_eval(model, F, y, test_size=0.25, random_state=0):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        F, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "\n",
    "    # Optional extras\n",
    "    try:\n",
    "        auc = roc_auc_score(y_te, model.predict_proba(X_te)[:, 1])\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "    cm = confusion_matrix(y_te, y_pred)\n",
    "\n",
    "    print(f\"[holdout] accuracy={acc:.3f} | f1={f1:.3f} | auc={auc if auc is not None else 'n/a'}\")\n",
    "    print(\"[holdout] confusion matrix:\\n\", cm)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"auc\": auc, \"cm\": cm}\n",
    "\n",
    "\n",
    "def demo_train_and_rank(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    shapelets: List[List[np.ndarray]],\n",
    "    *,\n",
    "    per_window_z: bool = True,\n",
    "    global_channel_z: bool = False,\n",
    "    random_state: int = 0,\n",
    "    n_perm_repeats: int = 10,\n",
    "    use_random_forest: bool = True,\n",
    "    rf_kwargs: Optional[dict] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build features, fit models, and compute feature importance.\n",
    "    Returns a dict with features, mapping, models, and importance arrays.\n",
    "    \"\"\"\n",
    "    # 1) Build features\n",
    "    F, specs = build_shapelet_features(\n",
    "        X, shapelets,\n",
    "        per_window_z=per_window_z,\n",
    "        global_channel_z=global_channel_z,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    results: Dict[str, Any] = {\"F\": F, \"specs\": specs}\n",
    "\n",
    "    # 2) Model A: Logistic Regression with L1 (sparse, interpretable weights)\n",
    "    logi = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LogisticRegressionCV(\n",
    "            Cs=20,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"liblinear\",\n",
    "            scoring=\"roc_auc\",\n",
    "            cv=5,\n",
    "            max_iter=2000,\n",
    "            n_jobs=None,\n",
    "            random_state=random_state,\n",
    "            refit=True,\n",
    "        )\n",
    "    )\n",
    "    logi.fit(F, y)\n",
    "    results[\"logistic_pipeline\"] = logi\n",
    "\n",
    "    # Extract absolute coefficients as a crude importance\n",
    "    lr = logi.named_steps[\"logisticregressioncv\"]\n",
    "    # coef_ shape (1, n_features) for binary → flatten\n",
    "    coef_abs = np.abs(lr.coef_.ravel())\n",
    "    results[\"coef_abs\"] = coef_abs\n",
    "    summarize_top_features(coef_abs, specs, k=20, title=\"L1-LogReg | |coef|\")\n",
    "\n",
    "    # 3) Model B (optional): Random Forest + impurity importance\n",
    "    if use_random_forest:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            **(rf_kwargs or {})\n",
    "        )\n",
    "        rf.fit(F, y)\n",
    "        results[\"rf\"] = rf\n",
    "        rf_imp = rf.feature_importances_\n",
    "        results[\"rf_importance\"] = rf_imp\n",
    "        summarize_top_features(rf_imp, specs, k=20, title=\"RandomForest | impurity importance\")\n",
    "\n",
    "    # 4) Permutation importance (model-agnostic). Use the better of the two models by AUC.\n",
    "    # Compute quick train AUCs to decide which model to permute on (not perfect, but simple).\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc_logi = roc_auc_score(y, logi.predict_proba(F)[:, 1])\n",
    "    model_for_perm = logi\n",
    "    model_name = \"LogReg\"\n",
    "    if use_random_forest:\n",
    "        auc_rf = roc_auc_score(y, rf.predict_proba(F)[:, 1])\n",
    "        if auc_rf > auc_logi:\n",
    "            model_for_perm = rf\n",
    "            model_name = \"RF\"\n",
    "\n",
    "    print(f\"\\n[perm] Using {model_name} for permutation importance (train AUC baseline).\")\n",
    "    perm = permutation_importance(\n",
    "        model_for_perm, F, y,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_repeats=n_perm_repeats,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    perm_mean = perm.importances_mean\n",
    "    perm_std = perm.importances_std\n",
    "    results[\"perm_mean\"] = perm_mean\n",
    "    results[\"perm_std\"] = perm_std\n",
    "    summarize_top_features(perm_mean, specs, k=20, title=\"Permutation importance | mean ΔAUC\")\n",
    "\n",
    "    eval_res = holdout_eval(logi, F, y, test_size=0.25, random_state=0)\n",
    "\n",
    "    return results, eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from models.Shapelet import ShapeBottleneckModel\n",
    "from data_provider.data_loader import UEAloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n"
     ]
    }
   ],
   "source": [
    "root_path = data_dir + 'FingerMovements'\n",
    "data = UEAloader(root_path, flag=\"TRAIN\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    epsilon: float = 1.0\n",
    "    distance_func: str = 'euclidean'\n",
    "    memory_efficient: bool = True\n",
    "    seq_len: int = 50\n",
    "    enc_in: int = 28\n",
    "    num_class: int = 2\n",
    "    pool: str = 'max'\n",
    "    sbm_cls: str = 'linear'\n",
    "    dropout: float = 0.0\n",
    "    lambda_div: float = 0.1\n",
    "    lambda_reg: float = 0.1\n",
    "\n",
    "config = Config()\n",
    "\n",
    "path_large = model_dir + 'SBM/FingerMovements/dnn-FCN_seed-0_k-10_div-0.1_reg-0.1_eps-1.0_beta-constant_dfunc-euclidean_cls-linear'\n",
    "                            \n",
    "model_large =  ShapeBottleneckModel(num_shapelet=[10, 10, 10, 10, 10, 10], shapelet_len=[0.05, 0.1, 0.2, 0.3, 0.5, 0.8], pool='max', configs=config).eval()\n",
    "model_large.load_state_dict(torch.load(f\"{path_large}/checkpoint.pth\"))\n",
    "\n",
    "shapelets = [[] for i in range(28)]\n",
    "for i in range(5):\n",
    "    sh = model_large.shapelets[i].weights\n",
    "    for j in range(10):\n",
    "        for ch in range(28):\n",
    "            shapelets[ch].append(sh[j, ch, :].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] samples=316, channels=28, time=50\n",
      "[build] total shapelets=1400, features=1400\n",
      "[build] distance = min Euclidean (raw)\n",
      "[build] channel 0: 50 shapelets\n",
      "[build] channel 1: 50 shapelets\n",
      "[build] channel 2: 50 shapelets\n",
      "[build] channel 3: 50 shapelets\n",
      "[build] channel 4: 50 shapelets\n",
      "[build] channel 5: 50 shapelets\n",
      "[build] channel 6: 50 shapelets\n",
      "[build] channel 7: 50 shapelets\n",
      "[build] channel 8: 50 shapelets\n",
      "[build] channel 9: 50 shapelets\n",
      "[build] channel 10: 50 shapelets\n",
      "[build] channel 11: 50 shapelets\n",
      "[build] channel 12: 50 shapelets\n",
      "[build] channel 13: 50 shapelets\n",
      "[build] channel 14: 50 shapelets\n",
      "[build] channel 15: 50 shapelets\n",
      "[build] channel 16: 50 shapelets\n",
      "[build] channel 17: 50 shapelets\n",
      "[build] channel 18: 50 shapelets\n",
      "[build] channel 19: 50 shapelets\n",
      "[build] channel 20: 50 shapelets\n",
      "[build] channel 21: 50 shapelets\n",
      "[build] channel 22: 50 shapelets\n",
      "[build] channel 23: 50 shapelets\n",
      "[build] channel 24: 50 shapelets\n",
      "[build] channel 25: 50 shapelets\n",
      "[build] channel 26: 50 shapelets\n",
      "[build] channel 27: 50 shapelets\n",
      "\n",
      "L1-LogReg | |coef| (top 20):\n",
      "# 1  f= 173  [ch=3, shp=23, L=10]  imp=1.347373\n",
      "# 2  f= 851  [ch=17, shp=1, L=3]  imp=1.135885\n",
      "# 3  f= 725  [ch=14, shp=25, L=10]  imp=1.097738\n",
      "# 4  f= 793  [ch=15, shp=43, L=25]  imp=0.965365\n",
      "# 5  f=   6  [ch=0, shp=6, L=3]  imp=0.929754\n",
      "# 6  f= 954  [ch=19, shp=4, L=3]  imp=0.898591\n",
      "# 7  f=  68  [ch=1, shp=18, L=5]  imp=0.886731\n",
      "# 8  f= 709  [ch=14, shp=9, L=3]  imp=0.810988\n",
      "# 9  f= 267  [ch=5, shp=17, L=5]  imp=0.810672\n",
      "#10  f=1325  [ch=26, shp=25, L=10]  imp=0.762001\n",
      "#11  f=  57  [ch=1, shp=7, L=3]  imp=0.735454\n",
      "#12  f= 302  [ch=6, shp=2, L=3]  imp=0.726795\n",
      "#13  f=1194  [ch=23, shp=44, L=25]  imp=0.687747\n",
      "#14  f=  11  [ch=0, shp=11, L=5]  imp=0.677836\n",
      "#15  f= 562  [ch=11, shp=12, L=5]  imp=0.660475\n",
      "#16  f= 568  [ch=11, shp=18, L=5]  imp=0.643308\n",
      "#17  f=1217  [ch=24, shp=17, L=5]  imp=0.638154\n",
      "#18  f= 604  [ch=12, shp=4, L=3]  imp=0.632586\n",
      "#19  f=  75  [ch=1, shp=25, L=10]  imp=0.580847\n",
      "#20  f= 353  [ch=7, shp=3, L=3]  imp=0.570279\n",
      "\n",
      "[perm] Using LogReg for permutation importance (train AUC baseline).\n",
      "\n",
      "Permutation importance | mean ΔAUC (top 20):\n",
      "# 1  f= 173  [ch=3, shp=23, L=10]  imp=0.127849\n",
      "# 2  f= 725  [ch=14, shp=25, L=10]  imp=0.116823\n",
      "# 3  f= 851  [ch=17, shp=1, L=3]  imp=0.114104\n",
      "# 4  f= 793  [ch=15, shp=43, L=25]  imp=0.100769\n",
      "# 5  f= 954  [ch=19, shp=4, L=3]  imp=0.085732\n",
      "# 6  f=  68  [ch=1, shp=18, L=5]  imp=0.083519\n",
      "# 7  f=   6  [ch=0, shp=6, L=3]  imp=0.078591\n",
      "# 8  f= 267  [ch=5, shp=17, L=5]  imp=0.071591\n",
      "# 9  f= 709  [ch=14, shp=9, L=3]  imp=0.069618\n",
      "#10  f= 302  [ch=6, shp=2, L=3]  imp=0.065472\n",
      "#11  f=  57  [ch=1, shp=7, L=3]  imp=0.060625\n",
      "#12  f=1325  [ch=26, shp=25, L=10]  imp=0.060284\n",
      "#13  f=  11  [ch=0, shp=11, L=5]  imp=0.053274\n",
      "#14  f=1194  [ch=23, shp=44, L=25]  imp=0.050104\n",
      "#15  f= 986  [ch=19, shp=36, L=15]  imp=0.049944\n",
      "#16  f= 604  [ch=12, shp=4, L=3]  imp=0.047585\n",
      "#17  f= 562  [ch=11, shp=12, L=5]  imp=0.044881\n",
      "#18  f=1217  [ch=24, shp=17, L=5]  imp=0.044396\n",
      "#19  f= 353  [ch=7, shp=3, L=3]  imp=0.041071\n",
      "#20  f=  75  [ch=1, shp=25, L=10]  imp=0.039579\n",
      "[holdout] accuracy=0.595 | f1=0.600 | auc=0.6057692307692307\n",
      "[holdout] confusion matrix:\n",
      " [[23 17]\n",
      " [15 24]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([x[0].numpy().T for x in data])\n",
    "y = np.array([x[1].item() for x in data])\n",
    "\n",
    "_ = demo_train_and_rank(\n",
    "    X, y, shapelets,\n",
    "    per_window_z=False,         # z-normalize each window + shapelet (recommended)\n",
    "    global_channel_z=False,    # or True, if you want an extra global channel z-norm\n",
    "    n_perm_repeats=8,\n",
    "    use_random_forest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "[build] samples=268, channels=6, time=896\n",
      "[build] total shapelets=300, features=300\n",
      "[build] distance = min Euclidean (raw)\n",
      "[build] channel 0: 50 shapelets\n",
      "[build] channel 1: 50 shapelets\n",
      "[build] channel 2: 50 shapelets\n",
      "[build] channel 3: 50 shapelets\n",
      "[build] channel 4: 50 shapelets\n",
      "[build] channel 5: 50 shapelets\n",
      "\n",
      "L1-LogReg | |coef| (top 20):\n",
      "# 1  f= 297  [ch=5, shp=47, L=448]  imp=7.230367\n",
      "# 2  f= 258  [ch=5, shp=8, L=45]  imp=3.856688\n",
      "# 3  f= 150  [ch=3, shp=0, L=45]  imp=2.672370\n",
      "# 4  f= 254  [ch=5, shp=4, L=45]  imp=2.660120\n",
      "# 5  f=  51  [ch=1, shp=1, L=45]  imp=2.543900\n",
      "# 6  f= 193  [ch=3, shp=43, L=448]  imp=2.314850\n",
      "# 7  f= 136  [ch=2, shp=36, L=269]  imp=2.029783\n",
      "# 8  f= 287  [ch=5, shp=37, L=269]  imp=1.443823\n",
      "# 9  f=  11  [ch=0, shp=11, L=90]  imp=1.237195\n",
      "#10  f=  37  [ch=0, shp=37, L=269]  imp=1.169815\n",
      "#11  f= 206  [ch=4, shp=6, L=45]  imp=0.904482\n",
      "#12  f=  26  [ch=0, shp=26, L=180]  imp=0.853962\n",
      "#13  f=   4  [ch=0, shp=4, L=45]  imp=0.651980\n",
      "#14  f= 158  [ch=3, shp=8, L=45]  imp=0.352565\n",
      "#15  f= 234  [ch=4, shp=34, L=269]  imp=0.338626\n",
      "#16  f= 207  [ch=4, shp=7, L=45]  imp=0.303232\n",
      "#17  f=  93  [ch=1, shp=43, L=448]  imp=0.273820\n",
      "#18  f= 167  [ch=3, shp=17, L=90]  imp=0.269546\n",
      "#19  f= 247  [ch=4, shp=47, L=448]  imp=0.260094\n",
      "#20  f= 154  [ch=3, shp=4, L=45]  imp=0.228929\n",
      "\n",
      "[perm] Using LogReg for permutation importance (train AUC baseline).\n",
      "\n",
      "Permutation importance | mean ΔAUC (top 20):\n",
      "# 1  f= 297  [ch=5, shp=47, L=448]  imp=0.434468\n",
      "# 2  f= 258  [ch=5, shp=8, L=45]  imp=0.283584\n",
      "# 3  f= 193  [ch=3, shp=43, L=448]  imp=0.167885\n",
      "# 4  f= 150  [ch=3, shp=0, L=45]  imp=0.164536\n",
      "# 5  f= 254  [ch=5, shp=4, L=45]  imp=0.142419\n",
      "# 6  f=  51  [ch=1, shp=1, L=45]  imp=0.136786\n",
      "# 7  f= 136  [ch=2, shp=36, L=269]  imp=0.131182\n",
      "# 8  f= 287  [ch=5, shp=37, L=269]  imp=0.097696\n",
      "# 9  f=  37  [ch=0, shp=37, L=269]  imp=0.077471\n",
      "#10  f=  11  [ch=0, shp=11, L=90]  imp=0.073169\n",
      "#11  f= 206  [ch=4, shp=6, L=45]  imp=0.052792\n",
      "#12  f=  26  [ch=0, shp=26, L=180]  imp=0.052680\n",
      "#13  f=   4  [ch=0, shp=4, L=45]  imp=0.031078\n",
      "#14  f= 234  [ch=4, shp=34, L=269]  imp=0.013965\n",
      "#15  f= 158  [ch=3, shp=8, L=45]  imp=0.013019\n",
      "#16  f= 154  [ch=3, shp=4, L=45]  imp=0.009900\n",
      "#17  f=  93  [ch=1, shp=43, L=448]  imp=0.008981\n",
      "#18  f= 207  [ch=4, shp=7, L=45]  imp=0.008960\n",
      "#19  f= 167  [ch=3, shp=17, L=90]  imp=0.007999\n",
      "#20  f= 247  [ch=4, shp=47, L=448]  imp=0.007964\n",
      "[holdout] accuracy=0.746 | f1=0.702 | auc=0.8119429590017826\n",
      "[holdout] confusion matrix:\n",
      " [[30  4]\n",
      " [13 20]]\n"
     ]
    }
   ],
   "source": [
    "root_path = data_dir + 'SelfRegulationSCP1'\n",
    "data = UEAloader(root_path, flag=\"TRAIN\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    epsilon: float = 1.0\n",
    "    distance_func: str = 'euclidean'\n",
    "    memory_efficient: bool = True\n",
    "    seq_len: int = 896\n",
    "    enc_in: int = 6\n",
    "    num_class: int = 2\n",
    "    pool: str = 'max'\n",
    "    sbm_cls: str = 'linear'\n",
    "    dropout: float = 0.0\n",
    "    lambda_div: float = 0.1\n",
    "    lambda_reg: float = 0.1\n",
    "\n",
    "config = Config()\n",
    "\n",
    "path_large = model_dir + 'SBM/SelfRegulationSCP1/dnn-FCN_seed-0_k-10_div-0.1_reg-0.1_eps-1.0_beta-constant_dfunc-euclidean_cls-linear'\n",
    "                            \n",
    "model_large =  ShapeBottleneckModel(num_shapelet=[10, 10, 10, 10, 10, 10], shapelet_len=[0.05, 0.1, 0.2, 0.3, 0.5, 0.8], pool='max', configs=config).eval()\n",
    "model_large.load_state_dict(torch.load(f\"{path_large}/checkpoint.pth\"))\n",
    "\n",
    "shapelets = [[] for i in range(6)]\n",
    "for i in range(5):\n",
    "    sh = model_large.shapelets[i].weights\n",
    "    for j in range(10):\n",
    "        for ch in range(6):\n",
    "            shapelets[ch].append(sh[j, ch, :].cpu().detach().numpy())\n",
    "\n",
    "X = np.array([x[0].numpy().T for x in data])\n",
    "y = np.array([x[1].item() for x in data])\n",
    "\n",
    "_ = demo_train_and_rank(\n",
    "    X, y, shapelets,\n",
    "    per_window_z=False,         # z-normalize each window + shapelet (recommended)\n",
    "    global_channel_z=False,    # or True, if you want an extra global channel z-norm\n",
    "    n_perm_repeats=8,\n",
    "    use_random_forest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[build] samples=200, channels=7, time=1152\n",
      "[build] total shapelets=350, features=350\n",
      "[build] distance = min Euclidean (raw)\n",
      "[build] channel 0: 50 shapelets\n",
      "[build] channel 1: 50 shapelets\n",
      "[build] channel 2: 50 shapelets\n",
      "[build] channel 3: 50 shapelets\n",
      "[build] channel 4: 50 shapelets\n",
      "[build] channel 5: 50 shapelets\n",
      "[build] channel 6: 50 shapelets\n",
      "\n",
      "L1-LogReg | |coef| (top 20):\n",
      "# 1  f=   0  [ch=0, shp=0, L=58]  imp=0.000000\n",
      "# 2  f= 349  [ch=6, shp=49, L=576]  imp=0.000000\n",
      "# 3  f= 348  [ch=6, shp=48, L=576]  imp=0.000000\n",
      "# 4  f= 347  [ch=6, shp=47, L=576]  imp=0.000000\n",
      "# 5  f= 346  [ch=6, shp=46, L=576]  imp=0.000000\n",
      "# 6  f= 345  [ch=6, shp=45, L=576]  imp=0.000000\n",
      "# 7  f= 344  [ch=6, shp=44, L=576]  imp=0.000000\n",
      "# 8  f= 343  [ch=6, shp=43, L=576]  imp=0.000000\n",
      "# 9  f= 342  [ch=6, shp=42, L=576]  imp=0.000000\n",
      "#10  f= 341  [ch=6, shp=41, L=576]  imp=0.000000\n",
      "#11  f= 340  [ch=6, shp=40, L=576]  imp=0.000000\n",
      "#12  f= 339  [ch=6, shp=39, L=346]  imp=0.000000\n",
      "#13  f= 338  [ch=6, shp=38, L=346]  imp=0.000000\n",
      "#14  f= 337  [ch=6, shp=37, L=346]  imp=0.000000\n",
      "#15  f= 336  [ch=6, shp=36, L=346]  imp=0.000000\n",
      "#16  f= 335  [ch=6, shp=35, L=346]  imp=0.000000\n",
      "#17  f= 334  [ch=6, shp=34, L=346]  imp=0.000000\n",
      "#18  f= 333  [ch=6, shp=33, L=346]  imp=0.000000\n",
      "#19  f= 332  [ch=6, shp=32, L=346]  imp=0.000000\n",
      "#20  f= 331  [ch=6, shp=31, L=346]  imp=0.000000\n",
      "\n",
      "[perm] Using LogReg for permutation importance (train AUC baseline).\n",
      "\n",
      "Permutation importance | mean ΔAUC (top 20):\n",
      "# 1  f=   0  [ch=0, shp=0, L=58]  imp=0.000000\n",
      "# 2  f= 349  [ch=6, shp=49, L=576]  imp=0.000000\n",
      "# 3  f= 348  [ch=6, shp=48, L=576]  imp=0.000000\n",
      "# 4  f= 347  [ch=6, shp=47, L=576]  imp=0.000000\n",
      "# 5  f= 346  [ch=6, shp=46, L=576]  imp=0.000000\n",
      "# 6  f= 345  [ch=6, shp=45, L=576]  imp=0.000000\n",
      "# 7  f= 344  [ch=6, shp=44, L=576]  imp=0.000000\n",
      "# 8  f= 343  [ch=6, shp=43, L=576]  imp=0.000000\n",
      "# 9  f= 342  [ch=6, shp=42, L=576]  imp=0.000000\n",
      "#10  f= 341  [ch=6, shp=41, L=576]  imp=0.000000\n",
      "#11  f= 340  [ch=6, shp=40, L=576]  imp=0.000000\n",
      "#12  f= 339  [ch=6, shp=39, L=346]  imp=0.000000\n",
      "#13  f= 338  [ch=6, shp=38, L=346]  imp=0.000000\n",
      "#14  f= 337  [ch=6, shp=37, L=346]  imp=0.000000\n",
      "#15  f= 336  [ch=6, shp=36, L=346]  imp=0.000000\n",
      "#16  f= 335  [ch=6, shp=35, L=346]  imp=0.000000\n",
      "#17  f= 334  [ch=6, shp=34, L=346]  imp=0.000000\n",
      "#18  f= 333  [ch=6, shp=33, L=346]  imp=0.000000\n",
      "#19  f= 332  [ch=6, shp=32, L=346]  imp=0.000000\n",
      "#20  f= 331  [ch=6, shp=31, L=346]  imp=0.000000\n",
      "[holdout] accuracy=0.500 | f1=0.603 | auc=0.4944\n",
      "[holdout] confusion matrix:\n",
      " [[ 6 19]\n",
      " [ 6 19]]\n"
     ]
    }
   ],
   "source": [
    "root_path = data_dir + 'SelfRegulationSCP2'\n",
    "data = UEAloader(root_path, flag=\"TRAIN\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    epsilon: float = 1.0\n",
    "    distance_func: str = 'euclidean'\n",
    "    memory_efficient: bool = True\n",
    "    seq_len: int = 1152\n",
    "    enc_in: int = 7\n",
    "    num_class: int = 2\n",
    "    pool: str = 'max'\n",
    "    sbm_cls: str = 'linear'\n",
    "    dropout: float = 0.0\n",
    "    lambda_div: float = 0.1\n",
    "    lambda_reg: float = 0.1\n",
    "\n",
    "config = Config()\n",
    "\n",
    "path_large = model_dir + 'SBM/SelfRegulationSCP2/dnn-FCN_seed-0_k-10_div-0.1_reg-0.1_eps-1.0_beta-constant_dfunc-euclidean_cls-linear'\n",
    "                            \n",
    "model_large =  ShapeBottleneckModel(num_shapelet=[10, 10, 10, 10, 10, 10], shapelet_len=[0.05, 0.1, 0.2, 0.3, 0.5, 0.8], pool='max', configs=config).eval()\n",
    "model_large.load_state_dict(torch.load(f\"{path_large}/checkpoint.pth\"))\n",
    "\n",
    "shapelets = [[] for i in range(7)]\n",
    "for i in range(5):\n",
    "    sh = model_large.shapelets[i].weights\n",
    "    for j in range(10):\n",
    "        for ch in range(7):\n",
    "            shapelets[ch].append(sh[j, ch, :].cpu().detach().numpy())\n",
    "\n",
    "X = np.array([x[0].numpy().T for x in data])\n",
    "y = np.array([x[1].item() for x in data])\n",
    "\n",
    "_ = demo_train_and_rank(\n",
    "    X, y, shapelets,\n",
    "    per_window_z=False,         # z-normalize each window + shapelet (recommended)\n",
    "    global_channel_z=False,    # or True, if you want an extra global channel z-norm\n",
    "    n_perm_repeats=8,\n",
    "    use_random_forest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bimai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
