{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class FeatureSpec:\n",
    "    channel: int\n",
    "    shapelet_idx: int\n",
    "    shapelet_len: int\n",
    "\n",
    "\n",
    "def _zscore(x: np.ndarray, axis=None, eps: float = 1e-8) -> np.ndarray:\n",
    "    m = x.mean(axis=axis, keepdims=True)\n",
    "    s = x.std(axis=axis, keepdims=True)\n",
    "    return (x - m) / (s + eps)\n",
    "\n",
    "\n",
    "def min_sliding_distance(\n",
    "    x: np.ndarray,\n",
    "    s: np.ndarray,\n",
    "    per_window_z: bool = False,\n",
    "    eps: float = 1e-8,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Minimal Euclidean distance between a 1D time series x (len T)\n",
    "    and a shapelet s (len L), computed over all T-L+1 windows.\n",
    "\n",
    "    If per_window_z=True, each window and the shapelet are z-normalized\n",
    "    before computing distances (useful for amplitude/offset invariance).\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    L = s.shape[0]\n",
    "    if L > T:\n",
    "        return np.inf  # cannot slide shapelet longer than the series\n",
    "\n",
    "    # rolling windows view: shape (T-L+1, L)\n",
    "    W = sliding_window_view(x, L)\n",
    "\n",
    "    if per_window_z:\n",
    "        # z-normalize each window (row-wise) and the shapelet once\n",
    "        Wn = _zscore(W, axis=1, eps=eps)\n",
    "        sn = _zscore(s, axis=0, eps=eps)\n",
    "        # broadcast subtract → (T-L+1, L)\n",
    "        d2 = np.square(Wn - sn).sum(axis=1)\n",
    "    else:\n",
    "        d2 = np.square(W - s).sum(axis=1)\n",
    "\n",
    "    return float(np.sqrt(d2.min()))\n",
    "\n",
    "\n",
    "def build_shapelet_features(\n",
    "    X: np.ndarray,                        # (n_samples, n_channels, n_timepoints)\n",
    "    shapelets: List[List[np.ndarray]],    # list over channels -> list of 1D arrays\n",
    "    *,\n",
    "    per_window_z: bool = True,\n",
    "    global_channel_z: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[np.ndarray, List[FeatureSpec]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      F: (n_samples, n_features) shapelet-distance features\n",
    "      specs: list mapping feature index -> (channel, shapelet_idx, shapelet_len)\n",
    "    \"\"\"\n",
    "    n_samples, n_channels, n_time = X.shape\n",
    "    assert len(shapelets) == n_channels, \"Provide shapelets per channel.\"\n",
    "\n",
    "    # Optional: z-normalize each channel globally per sample (not window-wise).\n",
    "    # This de-biases scale differences before the per-window computation.\n",
    "    Xn = X.copy()\n",
    "    if global_channel_z:\n",
    "        for i in range(n_samples):\n",
    "            for c in range(n_channels):\n",
    "                Xn[i, c] = _zscore(Xn[i, c])\n",
    "\n",
    "    # Precompute total features and a spec map\n",
    "    specs: List[FeatureSpec] = []\n",
    "    for c in range(n_channels):\n",
    "        for j, shp in enumerate(shapelets[c]):\n",
    "            specs.append(FeatureSpec(channel=c, shapelet_idx=j, shapelet_len=len(shp)))\n",
    "    n_features = len(specs)\n",
    "\n",
    "    if verbose:\n",
    "        total_shapelets = sum(len(lst) for lst in shapelets)\n",
    "        print(f\"[build] samples={n_samples}, channels={n_channels}, time={n_time}\")\n",
    "        print(f\"[build] total shapelets={total_shapelets}, features={n_features}\")\n",
    "        if per_window_z:\n",
    "            print(\"[build] distance = min Euclidean on z-scored windows\")\n",
    "        elif global_channel_z:\n",
    "            print(\"[build] distance = min Euclidean (channels globally z-scored)\")\n",
    "        else:\n",
    "            print(\"[build] distance = min Euclidean (raw)\")\n",
    "\n",
    "    F = np.empty((n_samples, n_features), dtype=np.float32)\n",
    "\n",
    "    # Compute features\n",
    "    # Feature index traverses channel-major then shapelet index\n",
    "    f_idx = 0\n",
    "    for c in range(n_channels):\n",
    "        S_c = shapelets[c]\n",
    "        if verbose:\n",
    "            print(f\"[build] channel {c}: {len(S_c)} shapelets\")\n",
    "        for j, shp in enumerate(S_c):\n",
    "            # ensure 1D float array\n",
    "            s = np.asarray(shp, dtype=float).ravel()\n",
    "            for i in range(n_samples):\n",
    "                x = np.asarray(Xn[i, c], dtype=float).ravel()\n",
    "                F[i, f_idx] = min_sliding_distance(x, s, per_window_z=per_window_z)\n",
    "            f_idx += 1\n",
    "\n",
    "    return F, specs\n",
    "\n",
    "\n",
    "def summarize_top_features(\n",
    "    importances: np.ndarray,\n",
    "    specs: List[FeatureSpec],\n",
    "    k: int = 20,\n",
    "    title: str = \"Top features\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    idx = np.argsort(importances)[::-1]  # descending\n",
    "    out = []\n",
    "    for r in idx[:k]:\n",
    "        spec = specs[r]\n",
    "        out.append({\n",
    "            \"rank\": len(out) + 1,\n",
    "            \"feature_index\": int(r),\n",
    "            \"channel\": spec.channel,\n",
    "            \"shapelet_idx\": spec.shapelet_idx,\n",
    "            \"shapelet_len\": spec.shapelet_len,\n",
    "            \"importance\": float(importances[r]),\n",
    "        })\n",
    "    print(f\"\\n{title} (top {min(k, len(importances))}):\")\n",
    "    for row in out:\n",
    "        print(\n",
    "            f\"#{row['rank']:>2}  f={row['feature_index']:>4}  \"\n",
    "            f\"[ch={row['channel']}, shp={row['shapelet_idx']}, L={row['shapelet_len']}]  \"\n",
    "            f\"imp={row['importance']:.6f}\"\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def holdout_eval(model, F, y, test_size=0.25, random_state=0):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        F, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "\n",
    "    # Optional extras\n",
    "    try:\n",
    "        auc = roc_auc_score(y_te, model.predict_proba(X_te)[:, 1])\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "    cm = confusion_matrix(y_te, y_pred)\n",
    "\n",
    "    print(f\"[holdout] accuracy={acc:.3f} | f1={f1:.3f} | auc={auc if auc is not None else 'n/a'}\")\n",
    "    print(\"[holdout] confusion matrix:\\n\", cm)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"auc\": auc, \"cm\": cm}\n",
    "\n",
    "\n",
    "def demo_train_and_rank(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    shapelets: List[List[np.ndarray]],\n",
    "    *,\n",
    "    per_window_z: bool = True,\n",
    "    global_channel_z: bool = False,\n",
    "    random_state: int = 0,\n",
    "    n_perm_repeats: int = 10,\n",
    "    use_random_forest: bool = True,\n",
    "    rf_kwargs: Optional[dict] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build features, fit models, and compute feature importance.\n",
    "    Returns a dict with features, mapping, models, and importance arrays.\n",
    "    \"\"\"\n",
    "    # 1) Build features\n",
    "    F, specs = build_shapelet_features(\n",
    "        X, shapelets,\n",
    "        per_window_z=per_window_z,\n",
    "        global_channel_z=global_channel_z,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    results: Dict[str, Any] = {\"F\": F, \"specs\": specs}\n",
    "\n",
    "    # 2) Model A: Logistic Regression with L1 (sparse, interpretable weights)\n",
    "    logi = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LogisticRegressionCV(\n",
    "            Cs=20,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"liblinear\",\n",
    "            scoring=\"roc_auc\",\n",
    "            cv=5,\n",
    "            max_iter=2000,\n",
    "            n_jobs=None,\n",
    "            random_state=random_state,\n",
    "            refit=True,\n",
    "        )\n",
    "    )\n",
    "    logi.fit(F, y)\n",
    "    results[\"logistic_pipeline\"] = logi\n",
    "\n",
    "    # Extract absolute coefficients as a crude importance\n",
    "    lr = logi.named_steps[\"logisticregressioncv\"]\n",
    "    # coef_ shape (1, n_features) for binary → flatten\n",
    "    coef_abs = np.abs(lr.coef_.ravel())\n",
    "    results[\"coef_abs\"] = coef_abs\n",
    "    summarize_top_features(coef_abs, specs, k=20, title=\"L1-LogReg | |coef|\")\n",
    "\n",
    "    # 3) Model B (optional): Random Forest + impurity importance\n",
    "    if use_random_forest:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            **(rf_kwargs or {})\n",
    "        )\n",
    "        rf.fit(F, y)\n",
    "        results[\"rf\"] = rf\n",
    "        rf_imp = rf.feature_importances_\n",
    "        results[\"rf_importance\"] = rf_imp\n",
    "        summarize_top_features(rf_imp, specs, k=20, title=\"RandomForest | impurity importance\")\n",
    "\n",
    "    # 4) Permutation importance (model-agnostic). Use the better of the two models by AUC.\n",
    "    # Compute quick train AUCs to decide which model to permute on (not perfect, but simple).\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc_logi = roc_auc_score(y, logi.predict_proba(F)[:, 1])\n",
    "    model_for_perm = logi\n",
    "    model_name = \"LogReg\"\n",
    "    if use_random_forest:\n",
    "        auc_rf = roc_auc_score(y, rf.predict_proba(F)[:, 1])\n",
    "        if auc_rf > auc_logi:\n",
    "            model_for_perm = rf\n",
    "            model_name = \"RF\"\n",
    "\n",
    "    print(f\"\\n[perm] Using {model_name} for permutation importance (train AUC baseline).\")\n",
    "    perm = permutation_importance(\n",
    "        model_for_perm, F, y,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_repeats=n_perm_repeats,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    perm_mean = perm.importances_mean\n",
    "    perm_std = perm.importances_std\n",
    "    results[\"perm_mean\"] = perm_mean\n",
    "    results[\"perm_std\"] = perm_std\n",
    "    summarize_top_features(perm_mean, specs, k=20, title=\"Permutation importance | mean ΔAUC\")\n",
    "\n",
    "    eval_res = holdout_eval(logi, F, y, test_size=0.25, random_state=0)\n",
    "\n",
    "    return results, eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "from utils import Data_Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 22:55:54,119 | INFO : Loading preprocessed data ...\n",
      "2025-10-22 22:55:54,122 | INFO : 158 samples will be used for training\n",
      "2025-10-22 22:55:54,123 | INFO : 158 samples will be used for validation\n",
      "2025-10-22 22:55:54,123 | INFO : 100 samples will be used for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] samples=158, channels=28, time=50\n",
      "[build] total shapelets=1680, features=1680\n",
      "[build] distance = min Euclidean (raw)\n",
      "[build] channel 0: 60 shapelets\n",
      "[build] channel 1: 60 shapelets\n",
      "[build] channel 2: 60 shapelets\n",
      "[build] channel 3: 60 shapelets\n",
      "[build] channel 4: 60 shapelets\n",
      "[build] channel 5: 60 shapelets\n",
      "[build] channel 6: 60 shapelets\n",
      "[build] channel 7: 60 shapelets\n",
      "[build] channel 8: 60 shapelets\n",
      "[build] channel 9: 60 shapelets\n",
      "[build] channel 10: 60 shapelets\n",
      "[build] channel 11: 60 shapelets\n",
      "[build] channel 12: 60 shapelets\n",
      "[build] channel 13: 60 shapelets\n",
      "[build] channel 14: 60 shapelets\n",
      "[build] channel 15: 60 shapelets\n",
      "[build] channel 16: 60 shapelets\n",
      "[build] channel 17: 60 shapelets\n",
      "[build] channel 18: 60 shapelets\n",
      "[build] channel 19: 60 shapelets\n",
      "[build] channel 20: 60 shapelets\n",
      "[build] channel 21: 60 shapelets\n",
      "[build] channel 22: 60 shapelets\n",
      "[build] channel 23: 60 shapelets\n",
      "[build] channel 24: 60 shapelets\n",
      "[build] channel 25: 60 shapelets\n",
      "[build] channel 26: 60 shapelets\n",
      "[build] channel 27: 60 shapelets\n",
      "\n",
      "L1-LogReg | |coef| (top 20):\n",
      "# 1  f= 526  [ch=8, shp=46, L=7]  imp=2.132734\n",
      "# 2  f= 646  [ch=10, shp=46, L=7]  imp=2.072079\n",
      "# 3  f= 392  [ch=6, shp=32, L=6]  imp=1.819387\n",
      "# 4  f= 812  [ch=13, shp=32, L=6]  imp=1.806570\n",
      "# 5  f= 872  [ch=14, shp=32, L=6]  imp=1.642485\n",
      "# 6  f=  86  [ch=1, shp=26, L=10]  imp=1.597300\n",
      "# 7  f= 757  [ch=12, shp=37, L=2]  imp=1.463842\n",
      "# 8  f= 759  [ch=12, shp=39, L=2]  imp=1.452477\n",
      "# 9  f=  75  [ch=1, shp=15, L=8]  imp=1.448700\n",
      "#10  f= 851  [ch=14, shp=11, L=10]  imp=1.415327\n",
      "#11  f=  90  [ch=1, shp=30, L=3]  imp=1.393264\n",
      "#12  f= 704  [ch=11, shp=44, L=2]  imp=1.385549\n",
      "#13  f=1037  [ch=17, shp=17, L=20]  imp=1.379721\n",
      "#14  f= 228  [ch=3, shp=48, L=2]  imp=1.317092\n",
      "#15  f=   5  [ch=0, shp=5, L=49]  imp=1.311545\n",
      "#16  f= 163  [ch=2, shp=43, L=6]  imp=1.302720\n",
      "#17  f= 886  [ch=14, shp=46, L=7]  imp=1.298331\n",
      "#18  f= 634  [ch=10, shp=34, L=7]  imp=1.258113\n",
      "#19  f=1280  [ch=21, shp=20, L=4]  imp=1.235755\n",
      "#20  f=1285  [ch=21, shp=25, L=3]  imp=1.223961\n",
      "\n",
      "[perm] Using LogReg for permutation importance (train AUC baseline).\n",
      "\n",
      "Permutation importance | mean ΔAUC (top 20):\n",
      "# 1  f= 526  [ch=8, shp=46, L=7]  imp=0.000240\n",
      "# 2  f= 646  [ch=10, shp=46, L=7]  imp=0.000160\n",
      "# 3  f= 812  [ch=13, shp=32, L=6]  imp=0.000120\n",
      "# 4  f= 392  [ch=6, shp=32, L=6]  imp=0.000100\n",
      "# 5  f= 872  [ch=14, shp=32, L=6]  imp=0.000020\n",
      "# 6  f= 553  [ch=9, shp=13, L=48]  imp=0.000000\n",
      "# 7  f= 554  [ch=9, shp=14, L=46]  imp=0.000000\n",
      "# 8  f= 555  [ch=9, shp=15, L=8]  imp=0.000000\n",
      "# 9  f= 556  [ch=9, shp=16, L=2]  imp=0.000000\n",
      "#10  f= 557  [ch=9, shp=17, L=20]  imp=0.000000\n",
      "#11  f= 558  [ch=9, shp=18, L=10]  imp=0.000000\n",
      "#12  f= 559  [ch=9, shp=19, L=13]  imp=0.000000\n",
      "#13  f= 560  [ch=9, shp=20, L=4]  imp=0.000000\n",
      "#14  f= 561  [ch=9, shp=21, L=4]  imp=0.000000\n",
      "#15  f= 562  [ch=9, shp=22, L=9]  imp=0.000000\n",
      "#16  f= 563  [ch=9, shp=23, L=20]  imp=0.000000\n",
      "#17  f= 564  [ch=9, shp=24, L=7]  imp=0.000000\n",
      "#18  f= 565  [ch=9, shp=25, L=3]  imp=0.000000\n",
      "#19  f= 566  [ch=9, shp=26, L=10]  imp=0.000000\n",
      "#20  f= 567  [ch=9, shp=27, L=2]  imp=0.000000\n",
      "[holdout] accuracy=0.600 | f1=0.636 | auc=0.595\n",
      "[holdout] confusion matrix:\n",
      " [[10 10]\n",
      " [ 6 14]]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'data_dir': data_dir + 'FingerMovements',\n",
    "    'Norm': False\n",
    "}\n",
    "Data = Data_Loader(config)\n",
    "X = Data['train_data']\n",
    "y = Data['train_label']\n",
    "\n",
    "with open('../store/FingerMovements_sd2.pkl', 'rb') as f:\n",
    "    sd = pickle.load(f)\n",
    "shapelets_info = sd.get_shapelet_info(number_of_shapelet=30)\n",
    "\n",
    "sw = torch.tensor(shapelets_info[:,3])\n",
    "sw = torch.softmax(sw*20, dim=0)*sw.shape[0]\n",
    "shapelets_info[:,3] = sw.numpy()\n",
    "\n",
    "shapelets = [[] for i in range(28)]\n",
    "for si in shapelets_info:\n",
    "    sc = X[int(si[0]), int(si[5]), int(si[1]):int(si[2])]\n",
    "    for i in range(28):\n",
    "        shapelets[i].append(sc)\n",
    "\n",
    "_ = demo_train_and_rank(\n",
    "    X, y, shapelets,\n",
    "    per_window_z=False,         # z-normalize each window + shapelet (recommended)\n",
    "    global_channel_z=False,    # or True, if you want an extra global channel z-norm\n",
    "    n_perm_repeats=8,\n",
    "    use_random_forest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 22:54:36,311 | INFO : Loading preprocessed data ...\n",
      "2025-10-22 22:54:36,328 | INFO : 134 samples will be used for training\n",
      "2025-10-22 22:54:36,328 | INFO : 134 samples will be used for validation\n",
      "2025-10-22 22:54:36,329 | INFO : 293 samples will be used for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] samples=134, channels=6, time=896\n",
      "[build] total shapelets=200, features=200\n",
      "[build] distance = min Euclidean (raw)\n",
      "[build] channel 0: 44 shapelets\n",
      "[build] channel 1: 34 shapelets\n",
      "[build] channel 2: 35 shapelets\n",
      "[build] channel 3: 25 shapelets\n",
      "[build] channel 4: 36 shapelets\n",
      "[build] channel 5: 26 shapelets\n",
      "\n",
      "L1-LogReg | |coef| (top 20):\n",
      "# 1  f= 137  [ch=3, shp=24, L=106]  imp=0.457578\n",
      "# 2  f= 108  [ch=2, shp=30, L=40]  imp=0.433581\n",
      "# 3  f= 197  [ch=5, shp=23, L=2]  imp=0.000000\n",
      "# 4  f= 199  [ch=5, shp=25, L=2]  imp=0.000000\n",
      "# 5  f= 195  [ch=5, shp=21, L=7]  imp=0.000000\n",
      "# 6  f= 194  [ch=5, shp=20, L=3]  imp=0.000000\n",
      "# 7  f= 193  [ch=5, shp=19, L=45]  imp=0.000000\n",
      "# 8  f= 198  [ch=5, shp=24, L=2]  imp=0.000000\n",
      "# 9  f= 192  [ch=5, shp=18, L=2]  imp=0.000000\n",
      "#10  f= 191  [ch=5, shp=17, L=2]  imp=0.000000\n",
      "#11  f= 189  [ch=5, shp=15, L=2]  imp=0.000000\n",
      "#12  f= 190  [ch=5, shp=16, L=2]  imp=0.000000\n",
      "#13  f= 187  [ch=5, shp=13, L=3]  imp=0.000000\n",
      "#14  f= 186  [ch=5, shp=12, L=39]  imp=0.000000\n",
      "#15  f= 185  [ch=5, shp=11, L=3]  imp=0.000000\n",
      "#16  f= 188  [ch=5, shp=14, L=56]  imp=0.000000\n",
      "#17  f= 183  [ch=5, shp=9, L=4]  imp=0.000000\n",
      "#18  f= 182  [ch=5, shp=8, L=4]  imp=0.000000\n",
      "#19  f= 181  [ch=5, shp=7, L=14]  imp=0.000000\n",
      "#20  f= 180  [ch=5, shp=6, L=9]  imp=0.000000\n",
      "\n",
      "[perm] Using LogReg for permutation importance (train AUC baseline).\n",
      "\n",
      "Permutation importance | mean ΔAUC (top 20):\n",
      "# 1  f= 137  [ch=3, shp=24, L=106]  imp=0.119179\n",
      "# 2  f= 108  [ch=2, shp=30, L=40]  imp=0.108957\n",
      "# 3  f= 197  [ch=5, shp=23, L=2]  imp=0.000000\n",
      "# 4  f= 199  [ch=5, shp=25, L=2]  imp=0.000000\n",
      "# 5  f= 195  [ch=5, shp=21, L=7]  imp=0.000000\n",
      "# 6  f= 194  [ch=5, shp=20, L=3]  imp=0.000000\n",
      "# 7  f= 193  [ch=5, shp=19, L=45]  imp=0.000000\n",
      "# 8  f= 198  [ch=5, shp=24, L=2]  imp=0.000000\n",
      "# 9  f= 192  [ch=5, shp=18, L=2]  imp=0.000000\n",
      "#10  f= 191  [ch=5, shp=17, L=2]  imp=0.000000\n",
      "#11  f= 189  [ch=5, shp=15, L=2]  imp=0.000000\n",
      "#12  f= 190  [ch=5, shp=16, L=2]  imp=0.000000\n",
      "#13  f= 187  [ch=5, shp=13, L=3]  imp=0.000000\n",
      "#14  f= 186  [ch=5, shp=12, L=39]  imp=0.000000\n",
      "#15  f= 185  [ch=5, shp=11, L=3]  imp=0.000000\n",
      "#16  f= 188  [ch=5, shp=14, L=56]  imp=0.000000\n",
      "#17  f= 183  [ch=5, shp=9, L=4]  imp=0.000000\n",
      "#18  f= 182  [ch=5, shp=8, L=4]  imp=0.000000\n",
      "#19  f= 181  [ch=5, shp=7, L=14]  imp=0.000000\n",
      "#20  f= 180  [ch=5, shp=6, L=9]  imp=0.000000\n",
      "[holdout] accuracy=0.706 | f1=0.750 | auc=0.8650519031141869\n",
      "[holdout] confusion matrix:\n",
      " [[ 9  8]\n",
      " [ 2 15]]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'data_dir': data_dir + 'SelfRegulationSCP1',\n",
    "    'Norm': False\n",
    "}\n",
    "Data = Data_Loader(config)\n",
    "X = Data['train_data']\n",
    "y = Data['train_label']\n",
    "\n",
    "with open('../store/SelfRegulationSCP1_sd2.pkl', 'rb') as f:\n",
    "    sd = pickle.load(f)\n",
    "shapelets_info = sd.get_shapelet_info(number_of_shapelet=100)\n",
    "\n",
    "sw = torch.tensor(shapelets_info[:,3])\n",
    "sw = torch.softmax(sw*20, dim=0)*sw.shape[0]\n",
    "shapelets_info[:,3] = sw.numpy()\n",
    "\n",
    "shapelets = [[] for i in range(6)]\n",
    "for si in shapelets_info:\n",
    "    sc = X[int(si[0]), int(si[5]), int(si[1]):int(si[2])]\n",
    "    shapelets[int(si[5])].append(sc)\n",
    "\n",
    "_ = demo_train_and_rank(\n",
    "    X, y, shapelets,\n",
    "    per_window_z=False,         # z-normalize each window + shapelet (recommended)\n",
    "    global_channel_z=False,    # or True, if you want an extra global channel z-norm\n",
    "    n_perm_repeats=8,\n",
    "    use_random_forest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 22:56:04,664 | INFO : Loading preprocessed data ...\n",
      "2025-10-22 22:56:04,673 | INFO : 100 samples will be used for training\n",
      "2025-10-22 22:56:04,674 | INFO : 100 samples will be used for validation\n",
      "2025-10-22 22:56:04,674 | INFO : 180 samples will be used for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] samples=100, channels=7, time=1152\n",
      "[build] total shapelets=1400, features=1400\n",
      "[build] distance = min Euclidean (raw)\n",
      "[build] channel 0: 200 shapelets\n",
      "[build] channel 1: 200 shapelets\n",
      "[build] channel 2: 200 shapelets\n",
      "[build] channel 3: 200 shapelets\n",
      "[build] channel 4: 200 shapelets\n",
      "[build] channel 5: 200 shapelets\n",
      "[build] channel 6: 200 shapelets\n",
      "\n",
      "L1-LogReg | |coef| (top 20):\n",
      "# 1  f=1313  [ch=6, shp=113, L=4]  imp=10.520085\n",
      "# 2  f= 591  [ch=2, shp=191, L=4]  imp=9.723508\n",
      "# 3  f=1367  [ch=6, shp=167, L=4]  imp=6.891448\n",
      "# 4  f=1113  [ch=5, shp=113, L=4]  imp=5.973976\n",
      "# 5  f= 907  [ch=4, shp=107, L=3]  imp=5.935302\n",
      "# 6  f=1107  [ch=5, shp=107, L=3]  imp=5.408041\n",
      "# 7  f= 100  [ch=0, shp=100, L=27]  imp=5.335140\n",
      "# 8  f=1170  [ch=5, shp=170, L=6]  imp=5.193987\n",
      "# 9  f= 858  [ch=4, shp=58, L=2]  imp=5.029822\n",
      "#10  f= 423  [ch=2, shp=23, L=5]  imp=5.029698\n",
      "#11  f= 991  [ch=4, shp=191, L=4]  imp=5.027256\n",
      "#12  f=1391  [ch=6, shp=191, L=4]  imp=5.014746\n",
      "#13  f=1191  [ch=5, shp=191, L=4]  imp=5.010266\n",
      "#14  f=1370  [ch=6, shp=170, L=6]  imp=4.849224\n",
      "#15  f=1018  [ch=5, shp=18, L=18]  imp=4.842071\n",
      "#16  f= 791  [ch=3, shp=191, L=4]  imp=4.780816\n",
      "#17  f=1308  [ch=6, shp=108, L=15]  imp=4.614746\n",
      "#18  f= 707  [ch=3, shp=107, L=3]  imp=4.517670\n",
      "#19  f= 376  [ch=1, shp=176, L=13]  imp=4.115450\n",
      "#20  f= 901  [ch=4, shp=101, L=25]  imp=4.061930\n",
      "\n",
      "[perm] Using LogReg for permutation importance (train AUC baseline).\n",
      "\n",
      "Permutation importance | mean ΔAUC (top 20):\n",
      "# 1  f=1313  [ch=6, shp=113, L=4]  imp=0.090550\n",
      "# 2  f=1018  [ch=5, shp=18, L=18]  imp=0.050900\n",
      "# 3  f= 591  [ch=2, shp=191, L=4]  imp=0.049400\n",
      "# 4  f=1113  [ch=5, shp=113, L=4]  imp=0.045300\n",
      "# 5  f=1170  [ch=5, shp=170, L=6]  imp=0.042600\n",
      "# 6  f= 907  [ch=4, shp=107, L=3]  imp=0.041650\n",
      "# 7  f=1368  [ch=6, shp=168, L=16]  imp=0.041050\n",
      "# 8  f= 423  [ch=2, shp=23, L=5]  imp=0.040800\n",
      "# 9  f=1370  [ch=6, shp=170, L=6]  imp=0.040500\n",
      "#10  f=1308  [ch=6, shp=108, L=15]  imp=0.040250\n",
      "#11  f=1216  [ch=6, shp=16, L=8]  imp=0.040200\n",
      "#12  f= 858  [ch=4, shp=58, L=2]  imp=0.040150\n",
      "#13  f=1107  [ch=5, shp=107, L=3]  imp=0.037150\n",
      "#14  f= 100  [ch=0, shp=100, L=27]  imp=0.036450\n",
      "#15  f=1098  [ch=5, shp=98, L=2]  imp=0.035700\n",
      "#16  f=1367  [ch=6, shp=167, L=4]  imp=0.035250\n",
      "#17  f=1391  [ch=6, shp=191, L=4]  imp=0.031950\n",
      "#18  f=1191  [ch=5, shp=191, L=4]  imp=0.031950\n",
      "#19  f= 991  [ch=4, shp=191, L=4]  imp=0.031950\n",
      "#20  f= 791  [ch=3, shp=191, L=4]  imp=0.031750\n",
      "[holdout] accuracy=0.360 | f1=0.385 | auc=0.2948717948717948\n",
      "[holdout] confusion matrix:\n",
      " [[4 9]\n",
      " [7 5]]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'data_dir': data_dir + 'SelfRegulationSCP2',\n",
    "    'Norm': False\n",
    "}\n",
    "Data = Data_Loader(config)\n",
    "X = Data['train_data']\n",
    "y = Data['train_label']\n",
    "\n",
    "with open('../store/SelfRegulationSCP2_sd2.pkl', 'rb') as f:\n",
    "    sd = pickle.load(f)\n",
    "shapelets_info = sd.get_shapelet_info(number_of_shapelet=100)\n",
    "\n",
    "sw = torch.tensor(shapelets_info[:,3])\n",
    "sw = torch.softmax(sw*20, dim=0)*sw.shape[0]\n",
    "shapelets_info[:,3] = sw.numpy()\n",
    "\n",
    "shapelets = [[] for i in range(7)]\n",
    "for si in shapelets_info:\n",
    "    sc = X[int(si[0]), int(si[5]), int(si[1]):int(si[2])]\n",
    "    for i in range(7):\n",
    "        shapelets[i].append(sc)\n",
    "\n",
    "_ = demo_train_and_rank(\n",
    "    X, y, shapelets,\n",
    "    per_window_z=False,         # z-normalize each window + shapelet (recommended)\n",
    "    global_channel_z=False,    # or True, if you want an extra global channel z-norm\n",
    "    n_perm_repeats=8,\n",
    "    use_random_forest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
